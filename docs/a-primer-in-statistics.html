<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 A primer in statistics" | Research Methods and GIS with R</title>
  <meta name="description" content="Course book for CORN276 Research Methods and GIS at Cornwall College Newquay" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 A primer in statistics" | Research Methods and GIS with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course book for CORN276 Research Methods and GIS at Cornwall College Newquay" />
  <meta name="github-repo" content="mbh038/rmgis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 A primer in statistics" | Research Methods and GIS with R" />
  
  <meta name="twitter:description" content="Course book for CORN276 Research Methods and GIS at Cornwall College Newquay" />
  

<meta name="author" content="Michael Hunt (with an enormous debt to Dylan Childs, from which this currently is mostly a straight copy)" />


<meta name="date" content="2021-06-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="installing-r-and-r-studio.html"/>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<style type="text/css">

.warning-box {
  border: 3px solid #e60000;
  margin:  25px;
  padding: 10px 10px 5px 10px;
  border-radius: 6px 6px 6px 6px;
}

.advanced-box {
  border: 3px solid #268bd2;
  margin:  25px;
  padding: 10px 10px 5px 10px;
  border-radius: 6px 6px 6px 6px;
}

.do-something {
  border: 3px solid #803e00;
  margin:  25px;
  padding: 10px 10px 5px 10px;
  border-radius: 6px 6px 6px 6px;
}

</style>








</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Course information and overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#why-do-this-course"><i class="fa fa-check"></i><b>1.1</b> Why do this course?</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.2</b> Course overview</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#aims"><i class="fa fa-check"></i><b>1.2.1</b> Aims</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#objectives"><i class="fa fa-check"></i><b>1.2.2</b> Objectives</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#assumed-background"><i class="fa fa-check"></i><b>1.2.3</b> Assumed background</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#methods"><i class="fa fa-check"></i><b>1.2.4</b> Methods</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#what-do-you-need-to-learn"><i class="fa fa-check"></i><b>1.2.5</b> What do you need to learn?</a></li>
<li class="chapter" data-level="1.2.6" data-path="index.html"><a href="index.html#what-is-required-of-you"><i class="fa fa-check"></i><b>1.2.6</b> What is required of you?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#how-to-use-the-teaching-material"><i class="fa fa-check"></i><b>1.3</b> How to use the teaching material</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#printed-material"><i class="fa fa-check"></i><b>1.3.1</b> The online course book</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#how-to-make-best-use-of-it"><i class="fa fa-check"></i><b>1.3.2</b> How to make best use of the teaching material</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#conventions"><i class="fa fa-check"></i><b>1.3.3</b> Conventions used in the course material</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i><b>1.3.4</b> Feedback</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#overall"><i class="fa fa-check"></i><b>1.3.5</b> Overall…</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#health-and-safety"><i class="fa fa-check"></i><b>1.4</b> Health and safety using display screen equipment</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="expected-learning.html"><a href="expected-learning.html"><i class="fa fa-check"></i><b>2</b> Expected learning outcomes</a>
<ul>
<li class="chapter" data-level="2.1" data-path="expected-learning.html"><a href="expected-learning.html#collecting-and-using-data"><i class="fa fa-check"></i><b>2.1</b> Collecting and using data</a></li>
<li class="chapter" data-level="2.2" data-path="expected-learning.html"><a href="expected-learning.html#statistical-concepts"><i class="fa fa-check"></i><b>2.2</b> Statistical Concepts</a></li>
<li class="chapter" data-level="2.3" data-path="expected-learning.html"><a href="expected-learning.html#simple-parametric-statistics"><i class="fa fa-check"></i><b>2.3</b> Simple parametric statistics</a></li>
<li class="chapter" data-level="2.4" data-path="expected-learning.html"><a href="expected-learning.html#regression-and-anova"><i class="fa fa-check"></i><b>2.4</b> Regression and ANOVA</a></li>
<li class="chapter" data-level="2.5" data-path="expected-learning.html"><a href="expected-learning.html#doing-more-with-models"><i class="fa fa-check"></i><b>2.5</b> Doing more with models</a></li>
<li class="chapter" data-level="2.6" data-path="expected-learning.html"><a href="expected-learning.html#experimental-design"><i class="fa fa-check"></i><b>2.6</b> Experimental design</a></li>
<li class="chapter" data-level="2.7" data-path="expected-learning.html"><a href="expected-learning.html#beyond-simple-models"><i class="fa fa-check"></i><b>2.7</b> Beyond simple models</a></li>
<li class="chapter" data-level="2.8" data-path="expected-learning.html"><a href="expected-learning.html#frequency-data-and-non-parameteric-tests"><i class="fa fa-check"></i><b>2.8</b> Frequency data and non-parameteric tests</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="installing-r-and-r-studio.html"><a href="installing-r-and-r-studio.html"><i class="fa fa-check"></i><b>3</b> Installing R and R Studio</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="installing-r-and-r-studio.html"><a href="installing-r-and-r-studio.html#getting-r"><i class="fa fa-check"></i><b>3.0.1</b> Getting R</a></li>
<li class="chapter" data-level="3.0.2" data-path="installing-r-and-r-studio.html"><a href="installing-r-and-r-studio.html#getting-rstudio"><i class="fa fa-check"></i><b>3.0.2</b> Getting RStudio</a></li>
<li class="chapter" data-level="3.0.3" data-path="installing-r-and-r-studio.html"><a href="installing-r-and-r-studio.html#what-to-open-and-what-not-to-open"><i class="fa fa-check"></i><b>3.0.3</b> What to open and what not to open</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="a-primer-in-statistics.html"><a href="a-primer-in-statistics.html"><i class="fa fa-check"></i><b>4</b> A primer in statistics"</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="a-primer-in-statistics.html"><a href="a-primer-in-statistics.html#populations-and-samples"><i class="fa fa-check"></i><b>4.0.1</b> Populations and samples</a></li>
<li class="chapter" data-level="4.0.2" data-path="a-primer-in-statistics.html"><a href="a-primer-in-statistics.html#three-types-of-variability-of-the-sample-of-the-population-and-of-the-estimate."><i class="fa fa-check"></i><b>4.0.2</b> Three types of variability: of the sample, of the population and of the estimate.</a></li>
<li class="chapter" data-level="4.0.3" data-path="a-primer-in-statistics.html"><a href="a-primer-in-statistics.html#confidence-intervals-a-way-of-precisely-representing-uncertainty"><i class="fa fa-check"></i><b>4.0.3</b> Confidence intervals: a way of precisely representing uncertainty</a></li>
<li class="chapter" data-level="4.0.4" data-path="a-primer-in-statistics.html"><a href="a-primer-in-statistics.html#the-big-hitch-with-confidence-intervals---why-we-use-a-t-distribution"><i class="fa fa-check"></i><b>4.0.4</b> The big hitch with confidence intervals - why we use a <span class="math inline">\(t\)</span>-distribution</a></li>
<li class="chapter" data-level="4.0.5" data-path="a-primer-in-statistics.html"><a href="a-primer-in-statistics.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.0.5</b> Hypothesis testing</a></li>
<li class="chapter" data-level="4.0.6" data-path="a-primer-in-statistics.html"><a href="a-primer-in-statistics.html#comparing-two-means"><i class="fa fa-check"></i><b>4.0.6</b> Comparing two means</a></li>
<li class="chapter" data-level="4.0.7" data-path="a-primer-in-statistics.html"><a href="a-primer-in-statistics.html#size-effects-vs-hypothesis-testing."><i class="fa fa-check"></i><b>4.0.7</b> Size effects vs hypothesis testing.</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Research Methods and GIS with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a-primer-in-statistics" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> A primer in statistics"</h1>
<p>Based on the very helpful revision chapter in <a href="https://global.oup.com/ukhe/product/modern-statistics-for-the-life-sciences-9780199252312?cc=gb&amp;lang=en&amp;">Modern Statistics for the Life Sciences, Alen Grafen and Rosie Hails, OUP</a>.</p>
<div id="populations-and-samples" class="section level3" number="4.0.1">
<h3><span class="header-section-number">4.0.1</span> Populations and samples</h3>
<p>It is rarely possible to get an exact answer to a question. Normally we have to make do with an estimate, and this may vary from a rough estimate to a more precise one.</p>
<p>One of the first tasks of statistics to state this in more precise terms.</p>
<p>Suppose, for example, we wanted to know the average height of adult women between the ages of 25 and 35 in the United Kingdom. It is quite impossible to measure the height of every single woman of that age. Instead we must content ourselves with taking a sample, finding the average height of women within that and hoping that it is representative of the whole population.</p>
<p>A <strong>sample</strong> is a <strong>random</strong> selection from within a <strong>population</strong>. For this to be the case the population needs to be precisely defined, since our sampling strategy will depend on this.</p>
<p>Having taken our sample we compute the mean by summing and dividing by the number of values:</p>
<p>//
{y}=
//</p>
<p>This is our estimate of the true population mean, <span class="math inline">\(\mu\)</span>. It is similar when we do an experiment. Measurements in an experiment inevitably involve error, and so we can think of the data readings we actually take as being a sample from the population of all the readings that could have occurred. The mean value that we get at the end from our data is thus an estimate of the true mean <span class="math inline">\(\mu_\text{A}\)</span>, which we would only have obtained if there was no error involved in the experiment.</p>
</div>
<div id="three-types-of-variability-of-the-sample-of-the-population-and-of-the-estimate." class="section level3" number="4.0.2">
<h3><span class="header-section-number">4.0.2</span> Three types of variability: of the sample, of the population and of the estimate.</h3>
<div id="variability-of-the-sample" class="section level4" number="4.0.2.1">
<h4><span class="header-section-number">4.0.2.1</span> Variability of the sample</h4>
<p>As well as the mean, we would like to know how variable our sample is. This gives us an idea as to how precise our estimate of the population mean is. Is it a rough one, or a precise one?</p>
<p>Two samples can have the same mean but very different variabilities. For example if the results for a class of <em>n</em>=30 students for a test in maths ranged from 40 to 70 while those for a test in English ranged from 50 to 60, then both might have a mean of 55, but, clearly, results in maths would be more variable than those in English.</p>
<p>For any individual score in either test, we can calculate its <strong>deviation</strong> from the mean of the score for that test, where</p>
<p><span class="math display">\[
\text{deviation} = \text{datapoint} - \text{mean}
\]</span>
If we sum the deviations of the scores from each test from their respective means, we would find that the absolute value of these deviations tend to be bigger for the maths test than for the English test. For both tests, however, the sum of the deviations would be zero, because of the definition of the mean as the central point, but if the deviations are squared and summed, we then get a measure of the variability of each dataset around its mean.</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-12-1.png" width="864" style="display: block; margin: auto;" /></p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Scores
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Mean
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Sum of deviations
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Sum of squared deviations
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Variance
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Standard deviation
</div>
</th>
</tr>
<tr>
<th style="text-align:center;">
</th>
<th style="text-align:center;">
<span class="math inline">\(\bar{y}\)</span>
</th>
<th style="text-align:center;">
</th>
<th style="text-align:center;">
</th>
<th style="text-align:center;">
<span class="math inline">\(s^2\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(s=\sqrt{s^2}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
English scores
</td>
<td style="text-align:center;">
<span class="math inline">\(\frac{1}{n}\Sigma y_i=55\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\Sigma \left(y_i-\bar{y}\right)=0\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\Sigma (y_i-\bar{y})^2=638\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\frac{1}{n-1}\Sigma (y_i-\bar{y})^2=\frac{638}{29}=22.0\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\sqrt{22.0}=4.7\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
maths scores
</td>
<td style="text-align:center;">
<span class="math inline">\(\frac{1}{n}\Sigma y_i=55\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\Sigma \left(y_i-\bar{y}\right)=0\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\Sigma (y_i-\bar{y})^2=2219\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\frac{1}{n-1}\Sigma (y_i-\bar{y})^2=\frac{2219}{29}=76.5\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\sqrt{76.5}=8.75\)</span>
</td>
</tr>
</tbody>
</table>
<p>In this case, a comparison of the sums of squares is valid, since the two datasets have the same size. In general though, a larger dataset will have a larger sum of squares, so for a valid comparison between unequally sized datasets, a measure that is independent of the size of the dataset is required.</p>
<p>To get this, all we need to do is take account of the sample size. We used <em>n</em> data points to define the mean, then the same <em>n</em> datapoints, plus the mean itself, to define the variability around the mean. But from the way in which the mean is calculated, the deviations must sum to zero. This means that we have only <em>n</em>-1 independent pieces of information about how the sample varies around the mean. Hence, our final measure of the variability of a data set, which we call the <strong>variance</strong> and denote as <span class="math inline">\(s^2\)</span>, is found by dividing the sum of squared deviations by <em>n</em>-1, not by <em>n</em>.</p>
<p><span class="math display">\[
s^2=\frac{\Sigma{\left(y_i-\bar{y}\right)^2}}{n-1}
\]</span></p>
<p>The number of independent pieces of information that contribute to the calculation of a statistic is called the <strong>degrees of freedom</strong>.</p>
<p>Often, we would like a measure of variability that has the same units as the data itself. The variance does not, but we remedy that by taking its square root to find the <strong>standard deviation</strong> <span class="math inline">\(s\)</span> of the dataset.</p>
</div>
<div id="variability-of-the-population" class="section level4" number="4.0.2.2">
<h4><span class="header-section-number">4.0.2.2</span> Variability of the population</h4>
<p>Just as we cannot know the true mean <span class="math inline">\(\mu\)</span> of a population, but can only estimate it from the mean <span class="math inline">\(\bar{y}\)</span> of a sample that we draw from that population, so we cannot know the true variance of a population. Nevertheless, it is useful to define it, and it is frequently referred to as <span class="math inline">\(\sigma^2\)</span> (“sigma squared”). Our best estimate of it is our sample variance <span class="math inline">\(s^2\)</span>.</p>
<p>A definition of it is that it is the expected squared deviation around the true mean for all individuals in the population.</p>
</div>
<div id="variability-of-the-estimate" class="section level4" number="4.0.2.3">
<h4><span class="header-section-number">4.0.2.3</span> Variability of the estimate</h4>
<p>Having obtained our estimate <span class="math inline">\(\bar{y}\)</span> of the true mean <span class="math inline">\(\mu\)</span> of a population, we would like to know how accurate it is. To answer this we will briefly discuss <strong>Normal distributions</strong>.</p>
<div id="the-standard-normal-distribution" class="section level5" number="4.0.2.3.1">
<h5><span class="header-section-number">4.0.2.3.1</span> The Standard Normal Distribution</h5>
<p>Many continuous attributes (eg weight, height, width) of a population are scattered around a mean value in such way that, if you plotted a histogram of the values, it would have a shape that is approximately bell-shaped and symmetric and well described by a Normal distribution (the reasons for this are interesting, but beyond the scope of this document). Such a distribution is described by two parameters - its mean (middle value) and its variance (spread). A <strong>standard normal</strong> distribution is simply a normal distribution with a mean of 0 and a variance (and therefore a standard deviation, which is the square root of the variance) of 1. Such a distribution is sometimes referred to as a <span class="math inline">\(Z\)</span> distribution.</p>
<p>Any normal distribution can be converted to a standard normal distribution by doing two things. Suppose a variable <span class="math inline">\(Y\)</span> follows a normal distribution, with mean 5 and standard deviation 2. First we subtract the mean from every value. This will have the effect of moving the whole distribution leftwards on the <em>x</em>-axis by 5 units, the mean value of <em>Y</em>, so that it is centred on 0. Then, we divide each value by 2, the standard deviation of <em>Y</em>. This will have the effect of squishing the distribution inwards, giving it a new standard deviation of 1. The result will be a standard normal, centred at 0, with standard deviation 1. The process of carrying out these two operations is known as <strong>standardising</strong>.</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-14-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>In summary, in order to convert a variable <em>Y</em> that is normally distributed with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, to a standard normal <em>z</em>, we subtract the mean then divide by the standard deviation:</p>
<p><span class="math display">\[
z=\frac{Y-\mu}{\sigma}
\]</span></p>
<p>Why would we want to do this? The answer is that the standard normal is an example of a <strong>probability density function</strong> (or <em>pdf</em> for short). Such functions have particular properties that are useful to us as scientists. In particular, it is straightforward to calculate what proportion of any set of observations described by a pdf fall within or beyond a certain number of standard deviations of the mean.</p>
<p>The total area under a standard normal, as for any pdf, is 1, since it is a certainty that the random variable it describes takes some value or other.</p>
<p>The area under it to the right of zero and the area under it to the left of zero are both 0.5, since the distribution is symmetric about zero. This tells you that if you had a random variable that was described by a standard normal, then there would be a 50% chance that it was positive, and a 50% chance that it was negative. In general, if you take a random individual from a population and measure the value of some attribute (such as its height) that is well described by a normal distribution, then there would be a 50% chance that the value for this individual is less than the population mean, and a 50% chance that it is greater.</p>
<p>The area under the distribution beyond a distance roughly two standard deviations (actually, 1.96) either side of the mean totals 0.05, or 5% of the total area under the curve. This means that if, again, we have a population for which some attribute is well described by a normal distribution, then roughly 95% of individuals will have a value of that attribute that falls within two standard deviations of the mean, and roughly 5% of them will fall beyond that. Or, put another way, if you took a random individual from the population, there is a 95% chance that its value for this attribute would be within about 2 standard deviations of the mean, and a 5% chance that it would be more than about 2 standard deviations greater or less than the mean.</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-15-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Of particular practical importance, if a dataset is normally distributed, then about 68% of the observations fall with one standard deviation of the mean, 95% fall within 1.96 standard deviations, about 96% fall with two standard deviations, and about 99.7% (ie practically all of them) fall within three standard deviations. The plot below illustrates this.</p>
<p><img src="rmgis_files/figure-html/fig.width-10-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>So much for an ideal normal distribution. A real data set drawn from a population that is approximately normally distributed would have some scatter, the more so the smaller the size of the sample. For such a sample we would find that <em>approximately</em> two thirds of the data set is within one standard deviation of the mean, 95% are within <em>approximately</em> two standard deviations and pretty much all of them are within three standard deviations.</p>
</div>
<div id="accuracy-of-the-estimate" class="section level5" number="4.0.2.3.2">
<h5><span class="header-section-number">4.0.2.3.2</span> Accuracy of the estimate</h5>
<p>We are interested in the population. We want to know its true mean <span class="math inline">\(\mu\)</span>, but what we have as our best estimate of this is the mean <span class="math inline">\(\bar{y}\)</span> of a sample of size <em>n</em> that we drew from the population. If we took another sample from the population of the same size, we would get a different sample mean, and so on again and again, if we had the time and resources to repeatedly take sample after sample. So our sample mean is itself a random variable <span class="math inline">\(\bar{Y}\)</span>, drawn from a population of all possible sample means. If we drew samples of the same size <em>n</em> many times from our population of interest, the means <span class="math inline">\(\bar{y}\)</span> of these samples would themselves form a distribution, the so-called <strong>sampling distribution</strong>, and the mean of <em>this</em> we would hope, would be the true mean <span class="math inline">\(\mu\)</span> of the population.</p>
<p>The bigger the variance <span class="math inline">\(\sigma^2\)</span> of the population, the more we would expect our estimate to differ from the true mean, and the less variable the population was, the closer we would expect our estimate to be to the true mean. Similarly, if we took a large sample then our estimate is likely to be closer to the true mean than if we took a small sample.</p>
<p>If we take these observations together, what we find is that the variance of the distribution of our estimates is <span class="math inline">\(\sigma^2/n\)</span>, and so the standard deviation of our estimate is the square root of this ie <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>. This is sometimes called the <strong>standard error of the mean</strong>. This gives us an idea of how precise our sample mean is as an estimate of the true mean.</p>
</div>
</div>
<div id="example" class="section level4" number="4.0.2.4">
<h4><span class="header-section-number">4.0.2.4</span> Example</h4>
<p>Suppose the population of grey seals around the coast of south west England includes 10,000 adult females, whose weights are normally distributed and in the range 100 - 190 kg. Let this be our ‘population of interest.’ The weights of individuals in this population are approximately normally distributed with a mean value of 145 kg, and a standard deviation of 15 kg.</p>
<p><em>Note that in reality we would not know the mean or standard deviation of this population, or how many seals there were in total or whether the weights of adult females within it were normally distributed (or distributed any other way).</em></p>
<p>A histogram of the weights of the entire population would look something like this:</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-18-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Suppose we wanted to know the mean and standard deviation of the weights of adult female grey seals in this population. Clearly, we could not find the true value since that would require weighing every seal in the population, which is impossible, but we could estimate the values by weighing all seals in a manageable sample that we hope is representative of the whole population. Suppose our sample were 100 randomly chosen adult female seals.</p>
<p>In reality, that would probably be the only sample we could get, and so our estimates <span class="math inline">\(\bar{y}\)</span> and <em>s</em> of the true mean <span class="math inline">\(\mu\)</span> and true standard deviation <span class="math inline">\(\sigma\)</span> respectively would be based entirely on that one sample.</p>
<p>To get an idea of how accurate our estimate is, imagine we could measure such samples of 100 seals randomly selected from the population many, many times over. Each sample would have a slightly different mean. Let us plot the distribution of some of those samples and superpose on top of them the ‘normal distribution’ curve that we know is a good representation of the weights of adult females in the whole population. (We know this because this is a simulation. In truth, we wouldn’t.) For each sample, we display its mean <span class="math inline">\(\bar{y}\)</span> and standard deviation <em>s</em>.</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-21-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Notice how all these sample distributions have <em>roughly</em> the form of a normal distribution but that each one is in detail different from the others. This is the reality of sampling from a population - every sample will be different - but not <em>completely</em> different. All those shown have roughly the same mean, shown by the dashed line, roughly the same standard deviation and roughly the same shape.</p>
<p>Note too that these samples are drawn from a population (we happen to know, because we created it!) whose mean value <span class="math inline">\(\mu\)</span> is 145 and for which the standard devation <span class="math inline">\(\sigma\)</span> is 15. In an actual study, we would have taken, most likely, just one sample, which could have been any of those you see above. The mean <span class="math inline">\(\bar{y}\)</span> and standard deviation <span class="math inline">\(s\)</span> of that sample would have been our best estimate of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<p>Below we contrast histograms of the population, a single sample of size 100 drawn from the population, and the so-called sampling distribution. That is, the distribution of the means of many samples of size 100 drawn from the population. Of these, the middle one, that of a single sample, is the only one we could get in practice.</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-23-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>In this case, the weights of the population are very close to being normally distributed. The mean is 145 kg and the standard deviation is 15 kg. You can see that the whole population has a weight within two or three standard deviations of the mean. In practice, we do not usually know either that the parameter of interest, in this case weight, definitely is normally distributed, or the mean and standard deviation of that distribution.</p>
<p>The weights of one sample of size 100 drawn from the population are also approximately normally distributed with a mean and standard deviation approximately equal to the that of the population. In practice, we might often only have only this one sample, so these would be our best estimates of the population mean and standard deviation and our judgement as to whether the population was normally distributed would be based on this one sample distribution alone. With small samples, it can often be hard to tell, just by looking at this histogram, whether the data have been drawn from a population that is normally distributed.</p>
<p>(In practice, we might also use other considerations - such as whether the data were a simple random sample and whether there were no outliers, and so on.)</p>
<p>The <strong>sampling distribution</strong> is in practice a hypothetical distribution, since we cannot normally take many samples, each here of size 100, find the mean of each and plot the distribution of these. But if we could, this is what we would get. The mean of the one sample that we actually got is somewhere within this distribution and the true mean of the population is at the centre of it. A very interesting and useful thing about this distribution is that it will very likely be normally distributed, even if the population distribution was not, provided the sample size is large enough, and it is narrower than the population distribution. The larger the sample size, the narrower it is. These are handy facts, since they together mean that its width gives us an idea of the precision of our sample mean as an estimate of the the true mean.</p>
<p>Some facts of interest about this distribution are:</p>
<ol style="list-style-type: lower-alpha">
<li><p>it is normally distributed (and, if the sample sizes are large enough and the samples independent of each other, it probably would be even if the underlying distribution of the population were not a normal distribution.</p></li>
<li><p>its mean is the true mean <span class="math inline">\(\mu\)</span> of the population.</p></li>
<li><p>its standard deviation is narrower than that of the population as a whole or of one sample. If the standard deviation of the population is <span class="math inline">\(\sigma\)</span>, where <span class="math inline">\(\sigma = 15\)</span>kg in this case, and the samples each had size <em>n</em>, where <em>n</em>=100 in this case, then the standard deviation of this sampling distribution is <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>. So in this case, the standard deviation of this distribution is 1/10th that of the underlying population.</p></li>
<li><p>if our sample size had been bigger, the sampling distribution would have been even narrower and so our estimate of the true mean would have been more precise. That is the benefit of having a bigger sample size.</p></li>
</ol>
<p>Now, here is the really interesting thing about this distribution. It tells us about the precision of our estimate <span class="math inline">\(\bar{y}\)</span> that we got from our one sample of 100 seals of the true population mean <span class="math inline">\(\mu\)</span>. Look at that sampling distribution. The true mean is somewhere in there, as is our sample mean. So whatever this value <span class="math inline">\(\bar{y}\)</span> is that we got from our sample, we know it is within the width of this distribution of the true mean. And how wide is this distribution? Well, since it reliably has the shape of a normal distribution, we know the answer to that. Roughly 95% of the values on this distribution are within two of its standard errors of the middle value <span class="math inline">\(\mu\)</span>. This standard error, remember, is <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>, where <span class="math inline">\(\sigma\)</span> is the population standard deviation, our best estimate of which is the standard deviation <span class="math inline">\(s\)</span> of our sample. Our sample mean <span class="math inline">\(\bar{y}\)</span> is our best estimate of this middle value, so we end up being able to say something like the following:</p>
<p><span class="math display">\[
\text{...the true mean = }\bar{y}{\text{ (our sample mean)}}\pm 2\times \frac{s{\text{ (our sample standard deviation)}}}{\sqrt{n}}
\]</span></p>
<p>We would call this range the <strong>95% confidence interval</strong> for the thing we wanted to measure - in this case, the mean weight of adult female grey seals in the population of them around the south west of England.</p>
<p>What we mean by this is that if we repeatedly took a sample of 100 seals from the population and constructed the confidence interval for the mean in this way, then the true value would be within the interval 95% of the time.</p>
<p>Let us explore confidence intervals in more detail…</p>
</div>
</div>
<div id="confidence-intervals-a-way-of-precisely-representing-uncertainty" class="section level3" number="4.0.3">
<h3><span class="header-section-number">4.0.3</span> Confidence intervals: a way of precisely representing uncertainty</h3>
<p>We know that our estimate <span class="math inline">\(\bar{y}\)</span> of the population mean <span class="math inline">\(\mu\)</span> comes from the distribution of all possible <span class="math inline">\(\bar{y}\)</span> that are distributed around <span class="math inline">\(\mu\)</span> with a variance of <span class="math inline">\(\frac{\sigma^2}{n}\)</span>, and thus a standard deviation of <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>. Let us now find the <strong>confidence interval</strong> from our data. This is the range of possible values for the true population mean (which we don’t know, remember) that cannot be rejected at the 5% significance level.</p>
<p>Parameters that have been estimated with great confidence will have a narrow confidence interval associated with them, while parameters about which we have less information will have a wide confidence interval.</p>
<p>From the properties of the standard normal distribution, we know that 95% of all such <span class="math inline">\(\bar{y}\)</span> will lie within 1.96 standard deviations of <span class="math inline">\(\mu\)</span>, where the relevant standard deviation is that of the sampling distribution - the distribution of <span class="math inline">\(\bar{y}\)</span>. That means that 5% will not!</p>
<p>This is illustrated below, where, for example, we show the true mean weight <span class="math inline">\(\mu\)</span> of adult female grey seals in south west England as a dotted line and either side of that, estimates of that obtained as the means from 20 samples, each of 100 seals, with their corresponding 95% confidence intervals. Note how nearly all of these confidence intervals do capture the true mean, but that one (in this case) does not.</p>
<p><img src="rmgis_files/figure-html/this_chunk-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Hence we can say that, for 96% of the time:</p>
<p><span class="math display">\[
\mu-2\frac{\sigma}{\sqrt{n}} \lt \bar{y} \lt \mu+2\frac{\sigma}{\sqrt{n}}
\]</span></p>
<p>In practice, by convention, we are interested in a confidence level of 95% rather than 96%. This changes the 2 in the above formula to 1.96 - the confidence level is slightly lower, so the confidence interval is slightly less wide. Further, we would rather instead state a confidence interval for <span class="math inline">\(\mu\)</span> in terms of <span class="math inline">\(\bar{y}\)</span>, rather than as above, so we rejig the last equation to give:</p>
<p><span class="math display">\[
\bar{y}-1.96\frac{\sigma}{\sqrt{n}} \lt \mu \lt \bar{y}+1.96\frac{\sigma}{\sqrt{n}}
\]</span></p>
<p>This is now our 95% confidence interval for data drawn from a normally distributed population: the range of values that the true mean <span class="math inline">\(\mu\)</span> could take and be consistent with the data at the 95% level.</p>
<p>But there is a hitch…</p>
</div>
<div id="the-big-hitch-with-confidence-intervals---why-we-use-a-t-distribution" class="section level3" number="4.0.4">
<h3><span class="header-section-number">4.0.4</span> The big hitch with confidence intervals - why we use a <span class="math inline">\(t\)</span>-distribution</h3>
<p>The trouble with the previous expression, as a way of stating the confidence interval for a parameter such as the mean of some measure of a population, is that it requires that we know <span class="math inline">\(\sigma\)</span>, the true standard deviation of of the population, and we don’t know it exactly. All we have is an estimate of it, ie <span class="math inline">\(s\)</span>, the standard deviation of the sample. So there is some uncertainty in our knowledge of <span class="math inline">\(\sigma\)</span>, just as there is in our knowledge of <span class="math inline">\(\mu\)</span> and this results in our 95% confidence interval for <span class="math inline">\(\mu\)</span> being somewhat wider than the value given above.</p>
<p>The way this extra uncertainty can be accommodated is by modelling our data not by a normal distribution, but by a <strong><em>t</em></strong>-<strong>distribution</strong>. This is similar to a normal distribution in that it is symmetrical, but it is lower and wider, with heavier tails on either side - which means that extreme values are more likely than for a normal. It is characterised by a centre, a scale and a <em>degrees of freedom</em> parameter <em>df</em> that can range from 1 to <span class="math inline">\(\infty\)</span> and which is one less than the number of data points in the sample: <em>df</em> =<em>n</em>-1. The precise shape of the <em>t</em>-distribution depends on <em>df</em>. For small <em>df</em> <em>t</em> distributions have very heavy tails, but as the sample size increases and <em>df</em> rises, so the <em>t</em>-distribution becomes taller and narrower and more and more like a normal distribution, until, for <em>df</em> greater than 30 or so, the two are more or less indistinguishable. This reflects the fact that, the more data points we have, the more precise our estimate <em>s</em> becomes of the population standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>This is illustrated below, where we see <em>t</em>-distributions for <em>df</em> = 1, 3, 10 and 30 against a standard normal distribution.</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-24-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>To calculate the 95% confidence interval now we need to know how many standard deviations of the <em>t</em>-distribution we need to go either side of the mean in order to encompass 95% of the population. We call this the <strong>critical t-value</strong> <span class="math inline">\(t_\text{crit}\)</span>. For a normal distribution, remember, we had to go 1.96 standard deviations either side in order to do this. For a <em>t</em>-distribution, how far we need to go will depend on the degrees of freedom <em>df</em>. For a low value of <em>df</em> the distribution has fatter tails so we need to go further out, but we need go less far as <em>df</em> increases and the <em>t</em>-distribution becomes narrower until, when <em>df</em> = 30 or so, we need only go as far as we would for a normal distribution, ie 1.96 standard deviations.</p>
<p>This is the effect of having a small sample: for such a sample our estimate of the true mean is less precise than if we had a larger sample, so the confidence interval, the range of values in which we are (say) 95% confident that the true value lies, is correspondingly wider.</p>
<p>For <em>df</em> = 10, we find that <span class="math inline">\(t_\text{crit}\)</span> = 2.228</p>
<p>So, now, for small samples, we would write our confidence interval as</p>
<p><span class="math display">\[
\bar{y}-t_\text{crit}\frac{s}{\sqrt{n}} \lt \mu \lt \bar{y}+t_\text{crit}\frac{s}{\sqrt{n}}
\]</span></p>
<p>or, put another way,</p>
<p><span class="math display">\[
\mu= \text{estimate}\pm t_\text{crit}\times\text{standard error of the estimate}
\]</span>
where the estimate is the mean of our sample, <em>s</em> is the standard deviation of the sample, <em>n</em> is the sample size and, for a 95% confidence interval and <em>df</em> = 10, <span class="math inline">\(t_\text{crit}\)</span> = 2.228. For other confidence levels or other values of <em>df</em>, <span class="math inline">\(t_\text{crit}\)</span> would have a different value.</p>
<div id="pros-and-cons-of-using-the-t-distribution." class="section level4" number="4.0.4.1">
<h4><span class="header-section-number">4.0.4.1</span> Pros and cons of using the <em>t</em>-distribution.</h4>
<p>The <em>t</em>-distribution is widely used as a way of calculating confidence intervals for population parameters from sample estimates. It can be used when the sample size is small, whereas the normal distribution cannot, but it can also be used when the sample size is large, which is really handy. However, it is only valid to use it when the sample comprises independent observations that have been drawn from a population that is normally distributed, and this is not always easy to tell for small samples, just when we would really like to use it.</p>
<p>For example the figures below show histograms of four samples, each of size 10, all drawn from the same normally distributed population. Would you be able to tell, from looking at these histograms, that this was the case?</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-25-1.png" width="864" style="display: block; margin: auto;" />
For such small samples, qq-plots are a better visual way to assess normality. Here are qq-plots for the same four samples.</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-26-1.png" width="864" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="hypothesis-testing" class="section level3" number="4.0.5">
<h3><span class="header-section-number">4.0.5</span> Hypothesis testing</h3>
<p>Here we lay out the concept of a null hypothesis and the method of testing such a hypothesis. We suppose that a sample mean and variance have been calculated, and that this information has been used to calculate a confidence interval. We can use this same information to test a hypothesis.</p>
<p>Suppose our sample was a set of 30 differences between two groups, for example the difference in test scores of a group before and after taking a statistics course. If there was no improvement over the duration of the course, then the mean difference should be zero. If the difference is defined as <em>score after</em> - <em>score before</em> then it is to be hoped that the mean difference is positive. However if the course actually confused the students then the difference could be negative.</p>
<p>To start with, we construct a <strong>null hypothesis</strong>. This normally expresses the conservative, ‘nothing going on’ scenario and states that no effect is expected, but it would be equally valid to state that the true mean takes some non-zero value.</p>
<p>On this case:</p>
<p><span class="math display">\[
\text{H}_0\text{: There is no difference between the scores, }\mu=0
\]</span>
The alternative is that there is a difference. Normally, we would not state the direction of this difference, so the alternative hypothesis is phrased as:</p>
<p><span class="math display">\[
\text{H}_\text{A}\text{: }\mu\neq0
\]</span>
The main principle of a hypothesis test is that we assume the null hypothesis is true and do not reject it unless there is convincing evidence that it is not true. In that sense it is like a classic court process, in which a defendant is assumed innocent and will be acquitted unless we find convincing evidence to the contrary.</p>
<p>Note that both the null and alternate hypotheses are phrased in terms of population parameters, since it is the populatiom that we want to know about. The sample that we have drawn from it is just our window onto that. The sample mean <span class="math inline">\(\bar{y}\)</span> will almost certainly not be zero, and even if it were it would not mean that the true mean <span class="math inline">\(\mu\)</span>, the mean of the population, is zero.</p>
<p>So what we do is assume that the null hypothesis is true and calculate the probability, given this, that we would have got the data we got, or more extreme data. By convention, if this probability falls below 0.05 we reject our assumption of <span class="math inline">\(\text{H}_0\)</span> being correct. This means that if the null hypothesis is true, there is a probability of 0.05 that we will reject it when we should not. We call this a <strong>Type 1</strong> error.</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-27-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>The figure above shows the distribution of our random variable (the mean difference between individual students’ scores before and after a course of study) under the null hypothesis. It is centred at zero, in this case. Our value of <span class="math inline">\(\bar{y}\)</span>, we suppose, is one datapoint from this distribution. In a hypothesis test we ask how likely it is that this could really be the case.</p>
<p>THe answer depends on how far from the centre of the distribution our value of <span class="math inline">\(\bar{y}\)</span> lies. If it is close to 0 then it may well have come from this distribution, but if it is far from it, then we conclude that it is unlikely to have done so.</p>
<p>When measuring the distance from 0, it is not the absolute distance that matters, but the number of standard errors of the sampling distribution, which we find by dividing the absolute distance by this standard error. In terms of this unit, this distance is known as the <strong><em>t</em>-statistic</strong>:</p>
<p><span class="math display">\[
t_\text{s}=\frac{\bar{y}-0}{\frac{s}{\sqrt{n}}}
\]</span>
Let us remember that the area under a probability distribution curve between two values is the probability that the random variable described by that pdf takes a value in that range. The probability that the mean of our sample could have been as far or further from 0 than it actually is is equal to the area under the distribution curve beyond that distance from 0, including both sides. We compare this with a critical probability, called the <strong>significance level</strong> of the test, <em>which we choose</em> but which is conventionally set at 0.05 ie 5% of the total area under the curve. The number of standard errors from 0 at which this happens is a critical value of the <em>t</em>-statistic known as <span class="math inline">\(t_\text{crit}\)</span>. Its value depends on the significance level we choose and on the degrees of freedom ie the sample size.</p>
<p>In the end, if our <em><em>t</em>-statistic</em> is greater than the critical <em>t</em>-value then the probability that we could have got such a value of <span class="math inline">\(\bar{y}\)</span> from a distribution centered on 0 is less than 0.05. We call this probability a <strong><em>p</em>-value</strong>, and so, if <span class="math inline">\(p&lt;0.05\)</span> we decide that the strength of the evidence is such as to allow us to <em>reject the null hypothesis</em>.</p>
<p>In summary, the <em>p</em>-value is the probability of obtaining the the data you got, and thus the <em>t</em>-statistic you got, if the null hypothresis were true.</p>
<div id="one-sample-t-test" class="section level4" number="4.0.5.1">
<h4><span class="header-section-number">4.0.5.1</span> One sample <em>t</em>-test</h4>
<p>Let us return to our class of students. Here is a histogram of the changes in their test scores following their course of study. We shall call this change DIFF:</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-28-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>We see that some students <em>did</em> score worse in the test following the course than in the test preceding it, but a majority have improved their score. It seems from this chart the the course of study has helped the students, on the whole, but to check that this improvement we carry out a test.</p>
<p>The mean value <span class="math inline">\(\bar{y}\)</span> of DIFF is 0.862 and the standard deviation <em>s</em> of DIFF is 0.838. The number of students is <em>n</em> is 30, so the degrees of freedom <em>df</em> is 29. The standard error in the mean is <span class="math inline">\(s/\sqrt{n} = 0.153\)</span>. Hence the <em>t</em>-statistic, the number of standard errors of the mean from the null prediction of 0 (in this case) is 0.862 / 0.153 = 5.64.</p>
<p>Look at the plot of a <em>t</em>-distribution for 30 degrees of the freedom (that for 29 degrees of freedom will be very similar to that) in the figure above. What proportion of the area under the curve, do you think, is more than 5.64 standard errors away from 0? Most of it it, some of it, or practically none of it?</p>
<p>You can see that this distance is so far way that there is effectively no area under the curve that is that far or further from 0. We interpret this as meaning there is almost zero probability that we would have got this data if the null hypothesis were true. This probability is what we call a <em>p</em>-value. In particular, the <em>p</em>-value for this one sample <em>t</em>-test is well below 0.05, in fact <em>p</em> &lt; 0.001 so we can confidently reject the null hypothesis and conclude that in general, students’ score did improve following their course of study. (We infer the direction of change from the fact that the mean difference is positive, and also from the range of values contained within the confidence interval.)</p>
<p>If we were to do this test in R, this is the output we would get:</p>
<pre><code>## 
##  One Sample t-test
## 
## data:  df$DIFF
## t = 5.6364, df = 29, p-value = 4.337e-06
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.5493389 1.1750558
## sample estimates:
## mean of x 
## 0.8621973</code></pre>
</div>
<div id="general-procedure-for-a-hypothesis-test" class="section level4" number="4.0.5.2">
<h4><span class="header-section-number">4.0.5.2</span> General procedure for a hypothesis test</h4>
<p>The procedure outlined above can be generalised to include a population mean of any value, not just zero, and to testing other parameters estimated from samples against hypothesised values of those parameters for the population.</p>
<p>The procedure can be broken down into these steps:</p>
<ol style="list-style-type: decimal">
<li>Define the null and alternate hypotheses in terms of population parameters.</li>
<li>Plot the data, most likely using a box plot or a histogram.</li>
<li>Calculate the sample estimate <span class="math inline">\(\bar{y}\)</span> of the population parameter.</li>
<li>Calculate the standard error <span class="math inline">\(s/\sqrt{n}\)</span> of this estimate.</li>
<li>Determine whether it is appropriate to use a <em>t</em>-test</li>
<li>Calculate the <em>t</em>-statistic</li>
<li>Calculate the <em>p</em>-value for this <em>t</em>-statistic.</li>
<li>Based on the <em>p</em>-value, reject or fail to reject the null hypothesis.</li>
</ol>
</div>
</div>
<div id="comparing-two-means" class="section level3" number="4.0.6">
<h3><span class="header-section-number">4.0.6</span> Comparing two means</h3>
<div id="a-two-sample-test-for-a-difference" class="section level4" number="4.0.6.1">
<h4><span class="header-section-number">4.0.6.1</span> A two-sample test for a difference</h4>
<p>Suppose we hypothesise that male and female squirrels differ in body mass. 50 squirrels of each sex are measured, and the body masses of each are recorded.</p>
<p>Histograms and qq-plots of the data are shown below:</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-33-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>These histograms, especially for males, do not look very symmetrical. Both distributions are skewed to the right. The effect of this is that the few squirrels with particularly large body masses will greatly increase the means of the samples and in doing so suggest that the whole body mass distribution is greater than it is in reality. This is reflected in the qqplots for both sexes, which are distinctly curved. The data are clearly not normally distributed.</p>
<p>As it stands, we cannot use a <em>t</em>-test to decide whether the data are drawn from the same distribution. We can instead either use a <em>non-parameteric</em> test for a difference, such as a Mann-Whitney test, that does not demand that the data follow a particular distribution, or we can attempt one of a number of possible <em>transformations</em> of the data, such as taking the natural log of the body mass, in the hope of achieving a more symmetric distribution.</p>
<p>we show the result of doing this below:</p>
<p><img src="rmgis_files/figure-html/unnamed-chunk-35-1.png" width="864" style="display: block; margin: auto;" />
That’s much better.
<span class="math display">\[
\begin{align*}
\text{H}_0&amp;:\quad\mu_\text{A}=\mu_\text{B}\quad\text{or}\quad\mu_\text{A}-\mu_\text{B}=0\\
\text{H}_0&amp;:\quad\mu_\text{A}\neq\mu_\text{B}
\end{align*}
\]</span></p>
</div>
</div>
<div id="size-effects-vs-hypothesis-testing." class="section level3" number="4.0.7">
<h3><span class="header-section-number">4.0.7</span> Size effects vs hypothesis testing.</h3>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="installing-r-and-r-studio.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
